---
title: LightGBM Explained
date: 2025-07-18 12:00:00 -500
categories: [random]
math: true
---

<style type="text/css">
.dots {
  text-align: center;
  font-size: 1.5em;
  letter-spacing: 2em;
  color: #777;
  margin: 1em;
}
</style>

## Summary

The authors of LightGBM claim it improves over earlier Gradient Boosted Tree implementations such as XGBoost and pGBRT by being way faster with a negligible cost in accuracy.

 More specifically, in traditional decision tree construction, for each feature a scan is necessary across all data instances to estimate the information gain for all possible split points, which is very time consuming. To tackle this problem, they propose two novel techniques: ***Gradient-based One-Side Sampling*** (GOSS) as well as ***Exclusive Feature Bundling*** (EFB).

With GOSS, they exclude a significant proportion of the data instances with small gradients, and only use the rest to compute information gain. They prove that, since the data instances with larger gradients play a more important role in information gain, GOSS can obtain accurate estimations of the information gain with a much smaller data batch size.

With EFB, they bundle mutually exclusive features (i.e. features who rarely both take on non-zero values simultaneously), to reduce the number of features that have to be considered.

## A Review of Gradient Boosted Trees

GBDT is an ensemble model of decision trees, which are trained in sequence. In each iteration, a new tree is constructed to predict the negative gradient (typically the residual error) of it's previous prediction compared to a target value. By then adding this negative gradient (multiplied by a learning rate) to the previous output, it's new prediction is moved closer to the target value, i.e. gradient descent.

The main cost in GBDT lies in learning the decision trees, and the most costly part of learning a decision tree is evaluating every possible split point that can be made across all features for maximum information gain. 

One of the most popular algorithms for finding candidate split points within a dataset is the presorted algorithm, which for each feature, will sort them in increasing order, and uses the halfway point between adjacent elements as candidates. The time complextiy for this is $O(\text{#data} \times \text{#features})$.

Datasets can commonly have millions of rows and dozens of features, so this split point evaluation can get very costly!

### Histogram-based GBDT Algorithm

Another popular algorithm is the histogram-based algorithm as shown for reference in Alg. 1 of the LightGBM paper, where instead of finding split points along the sorted feature values, the histogram-based algorithm buckets continuous feature values into discrete bins which each contain an equal amount of samples (i.e. quantile binning) and uses these bins to construct histograms during training. The algorithm is presented below:

![Alt Text](assets/lgbm/img/alg1.png)

`nodeSet` contains pointers to the leaves in the current level. On the first iteration, the only node contained is the root which itself is a leaf, it averages the targets in the dataset to use as its prediction. The tree will grow level by level up to `d` times, at which point the max depth of the tree is reached and it will stop growing.

`rowSet` contains a mapping of nodes to data indices depending on whether or not the data points hold the criteria to end up at that node.

- **For each `node` in `nodeSet`:**
    - `usedRows` is assigned to the subset of the data that has a trajectory in the tree that lands at this node, which was precomputed and is found by indexing into rowSet with node.
    - **For each feature `k`**, we build a histogram:
        - Declare a new variable `H` and initialize it to an empty histogram. 
        - **For each sample `j` in usedRows**, we populate the histogram:
            - Get the bin index for feature `k` on sample `j` and store it in `bin`, i.e., `bin ← I.f[k][j].bin`.
            - Increase the total sum of accumulated gradients within this bin - `H[bin].y` - by the gradient of the current sample, `I.y[j]`.
        - **Find the best split point on `k`.** Just like how split points were found previously for candidates at individual samples, we do it now at the border of histogram bins.
    - **Update `rowSet` and `nodeSet`** according to the best split point across all features, and continue to the next iteration.

It costs $O(\text{#data} \times \text{#features})$ for histogram building and $O(\text{#bins} \times \text{#features})$ for split point finding. Since $\text{#bins}$ is usually much smaller than $\text{#data}$, histogram building will dominate the computational complexity. If we can reduce $\text{#data}$ or $\text{#features}$, it'll substantially speed up the training of GBDT.

<div class="dots">···</div>

The large-scale datasets used in real world applications are typically quite sparse. GBDT with the pre-sorted algorithm can take advantage of this to reduce it's training cost by simply ignoring the features with zero values. However, GBDT with the histogram-based algorithm cannot take advantage of this sparse property for as  it needs to retrieve feature bin values (refer to Alg. 1) for each data point no matter if the feature value is zero or not. It would be great if the GBDT with the histogram-based algorithm could take advantage of this sparsity as well.

To address this, the authors introduce ***Gradient-based One-Side Sampling*** (GOSS) and ***Exclusive Feature Bundling*** (EFB).

## Gradient-based One-Side Sampling

The point of GOSS is to reduce $\text{#data}$ . It works by simply ignoring data instances who's previous gradients (AKA residuals) were small, the reasoning being that data instances with small gradients are already well trained and don't need to be focused on. The only problem with this is that ignoring these instances will shift the data distribution and can cause the model to overfit to the new distribution, while "forgetting" what it knew of previous data instances.

To address this GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients. In order to compensate for the influence to the distribution, when computing information gain, GOSS introduces a constant multiplier for the data instances with small gradients. Specifically GOSS first sorts the data according to the absolute value of their gradients, and selects the first $a \times 100\%$ instances. Then it randomly samples $b \times 100\%$ instances from the rest of the data. GOSS then amplifies the lower gradient samples by $\frac{1-a}{b}$ when calculating information gain. This allows to put more focus on under-trained instances without changing the original data distribution by much.

## Theoretical Analysis

GBDT uses decision trees to learn a function from the input space $X^s$ into the gradient space $G$. Supposed we have a set with $n$ i.i.d. instances $\\{x_1, \cdots, x_n \\}$, where each $x_i$ is a vector with dimension $s$ in the space $X^s$. In each iteration of gradient boosting, the negative gradients (residuals) of the loss function with respect to the output of the model are denoted as $\\{g_1, \cdots, g_n\\}$. The decision tree model splits each node at the most informative feature (with the largest information gain). For GBDT, the information gain is usually measured by the variance after splitting which is defined below.

**Definition 1.** *Let $O$ be the training dataset on a fixed node of the decision tree. The variance gain of splitting feature $j$ at point $d$ for this node is defined as*

$$
V_{j|O}(d) = \frac{1}{n_O} \left(\frac{(\sum_{\{ x_i \in O : x_{ij} \leq d \}} g_i)^2}{n^j_{l | O}(d)} + \frac{(\sum_{\{ x_i \in O : x_{ij} > d \}} g_i)^2}{n^j_{r | O}(d)} \right)
$$

*Where*

$$
\begin{align*}
n_O &= \sum_{i} I[x_i \in O], \\
n^j_{l|O}(d) &= \sum_{i} I[x_i \in O : x_{ij} \leq d], \\
n^j_{r|O}(d) &= \sum_{i} I[x_i \in O : x_{ij} > d]
\end{align*}
$$

For feature $j$, the decision tree algorithm selects $d^\*_j = \operatorname{argmax}_d V_j(d)$ and calculates the largest gain $V_j(d^\*_j)$. Then, the data is split according to the feature $j^\*$ at point $d\_{j^\*}$ into the left and right child nodes.

In the proposed GOSS method, first, training instances are ranked according to their absolute values of the gradients in descending order; second, the top $a \times 100\%$ instances are kept and get an instance subset $A$; then, for the remaining set $A^c$ consisting of $(1-a) \times 100\%$ instances with smaller gradients, another instance subset $B$ is randomely sampled with a size of $b \times \|A^c\|$; finally, the instances are split according to the estimated variance gain $\tilde{V}_j(d)$ over the subset $A \cup B$, i.e.,

$$
\tilde{V}_j(d) = \frac{1}{n} \left( \frac{(\sum_{ x_i \in A_l } g_i + \frac{1-a}{b}\sum_{ x_i \in B_l } g_i)^2}{n^j_l(d)} + \frac{(\sum_{ x_i \in A_r } g_i + \frac{1-a}{b}\sum_{ x_i \in B_r } g_i)^2}{n^j_r(d)} \right)
$$

*Where*

$$
\begin{align*}
A_l &= \{ x_i \in A : x_{ij} \leq d \} \\
A_r &= \{ x_i \in A : x_{ij} > d \} \\
B_l &= \{ x_i \in B : x_{ij} \leq d \} \\
B_r &= \{ x_i \in B : x_{ij} > d \} \\
\end{align*}
$$

Thus in GOSS, we use the estimated $\tilde{V}_j(d)$ over a smaller instance subset, instead of accurate $\tilde{V}_j(d)$ over all the instances to determine the split point, and the computation cost can be largely reduced. More importantly, the following theorem indicates that GOSS will not lose much training accuracy and will outperform random sampling. Due to space restrictions, the proof is left in the supplementary materials.

**Theorem 2** *We denote the approximation error in GOSS as* 

$$
\begin{align*}
\mathcal{E}(d) &= |\tilde{V}_j(d) - V_j(d)| \\[12pt]
g^{-j}_l(d) &= \frac{\sum_{x_i \in (A \cup A^c)_l} |g_i|}{n^j_l(d)} \\[11pt]
g^{-j}_r(d) &= \frac{\sum_{x_i \in (A \cup A^c)_r} |g_i|}{n^j_r(d)}
\end{align*}
$$

*With probability $1 - \delta$ we have*

$$
\mathcal{E}(d) \leq C^2_{a, b} \ln 1/\delta \cdot \max\left\{\frac{1}{n^j_l(d)}, \frac{1}{n^j_r(d)}\right\} + 2DC_{a,b}\sqrt\frac{\ln 1/\delta}{n}
$$

*Where* $C_{a,b} = \frac{1-a}{\sqrt{b}}\max_{x_i \in A^c}\|g_i\|$ *and* $D = \max(g^{-j}_l(d), g^{-j}_r(d)).$

According to the theorem, we have the following discussions: (1) The approximation ratio of GOSS is $\mathcal{O\left(\frac{1}{n^j_l(d)} + \frac{1}{n^j_r(d)} + \frac{1}{\sqrt{n}}\right)}$. If the split is not too unbalanced (i.e., $n^j_l \geq \mathcal{O}(\sqrt{n})$ and $n^j_r \geq \mathcal{O}(\sqrt{n})$), the approximation error will be dominated by the second term of Inequ.(2) which decreases to 0 in $\mathcal{O}(\sqrt{n})$ with $n \rightarrow \infty$. That means that when there's a lot of data, the approximation is quite accurate. (2) Random sampling is a special case of GOSS with $a = 0$. In many cases, GOSS could outperform random sampling, under the condition $C_{0, \beta} > C_{a, \beta-a}$, which is equivalent to $\frac{\alpha_a}{\sqrt{\beta}} > \frac{1 - a}{\sqrt{\beta - a}}$ with $\alpha_a = \max_{x_i \in A \cup A^c} \|g_i\|$.

Next, we analyze the generalization performance in GOSS. We consider the generalization error in GOSS $\mathcal{E}^\text{GOSS}_\text{gen}(d) = \|V_j(d) - V\_*(d) \|$, which is the gap between the variance gain calculated by the sampled training instances in GOSS and the true variance gain for the underlying distribution. We have $\mathcal{E}^\text{GOSS}\_\text{gen} \leq \|\tilde{V}_j(d) - V_j(d) \| + \|V_j(d) - V\_\*(d) \| \triangleq \mathcal{E}\_\text{GOSS}(d) + \mathcal{E}\_\text{gen}(d)$. Thus, the generalization error with GOSS will be close to that calculated using the full data instances if the GOSS approximation is accurate. On the other hand, sampling will increase the diversity of the base learners, which potentially help to improve generalization performance.